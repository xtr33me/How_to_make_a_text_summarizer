{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pickle as pickle\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import re\n",
    "import os\n",
    "import json\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#nltk.download()\n",
    "cachedStopWords = stopwords.words(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    try:\n",
    "        min_length = 3\n",
    "        words = map(lambda word: word.lower(), word_tokenize(text))#.decode('utf8', 'ignore')));\n",
    "        '''tokens = (list(map(lambda token: PorterStemmer().stem(token),\n",
    "                        words)));'''\n",
    "        p = re.compile('[a-zA-Z]+');\n",
    "        filtered_tokens = list(filter(lambda token:\n",
    "                            p.match(token) and len(token)>=min_length,\n",
    "                            words));\n",
    "        return filtered_tokens\n",
    "    except RuntimeError as e:\n",
    "        print (\"Runtime Error in tokenize: {} : {}\".format(e.errno, e.strerror) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    filename = \"./data/tokens.pkl\"\n",
    "    PATH = \"./data/\"#signalmedia-1m.jsonl\"\n",
    "    train_docs = []\n",
    "    test_docs = []\n",
    "    titles = []\n",
    "    descriptions = []\n",
    "    keywords = None\n",
    "\n",
    "    try:\n",
    "        for path, dirs, files in os.walk(PATH):\n",
    "            for fn in files:\n",
    "                fullpath = os.path.join(path, fn)\n",
    "                if fn != \".DS_Store\" and os.path.isfile(fullpath):\n",
    "                    print (\"FN: \" + fullpath)\n",
    "                    with open(fullpath) as f:\n",
    "                        train_docs = list(f) #json.load(f)\n",
    "                        #t = train_docs[0]#json.loads(train_docs[0])\n",
    "                        #print (t['id'])\n",
    "                        #train_docs.append(content)\n",
    "\n",
    "        \n",
    "        #Now we want to split our title from the content and tokenize\n",
    "        for sent in train_docs:\n",
    "            #print sent\n",
    "            article = json.loads(sent)\n",
    "            #article = sent.split(\"\\n\", 1)\n",
    "            #print (\"Article: \" + article['title'] + \" : \" + article['content'])\n",
    "            tit = \" \".join(tokenize(article['title']))\n",
    "            bod = \" \".join(tokenize(article['content']))\n",
    "            \n",
    "            titles.append(tit)\n",
    "            descriptions.append(bod)\n",
    "            \n",
    "\n",
    "        with open(filename, \"wb\") as f:\n",
    "            #print (descriptions[0])\n",
    "            pickle.dump((titles,descriptions,keywords), f, -1)\n",
    "\n",
    "        #fileobj = open(filename, 'wb')\n",
    "        #pickle.dump((titles,descriptions,keywords), fileobj)\n",
    "        #fileobj.close()\n",
    "        #heads, desc, keywords\n",
    "\n",
    "        # a = [1,2]\n",
    "        # b = [3,4]\n",
    "\n",
    "        # with open(\"tmp.pickle\", \"wb\") as f:\n",
    "        #     pickle.dump((a,b), f)\n",
    "\n",
    "        # with open(\"tmp.pickle\", \"rb\") as f:\n",
    "        #     a,b = pickle.load(f) \n",
    "\n",
    "    except RuntimeError as e:\n",
    "        print (\"Runtime Error: {} : {}\".format(e.errno, e.strerror))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FN: ./data/signalmedia-1m.jsonl\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
